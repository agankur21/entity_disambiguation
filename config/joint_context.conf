export MODEL_NAME='joint-context'
export text_encoder='lstm'
export optimizer='adam'
export loss_type='softmax'
export non_linearity='tanh'
export lr=.001   #learning rate
export epsilon=1e-8  #epsilon for adam optimizer
export margin=1.0   #margin for hinge loss
export l2_weight=0.0005 #weight for L2 loss
export clip_norm=10 #clip gradients to have norm <= this

export word_dropout=0.85
export lstm_dropout=0.85
export final_dropout=1.0

export text_batch=128  #Batch size
export neg_samples=5      #Number of negative samples per positive example

export token_dim=100  #Dimension of word2vec embedding
export mention_dim=100
export lstm_dim=2048
export embed_dim=100
export entity_dim=100
export final_out_dim=100

export text_epochs=100
export max_decrease_epochs=10
export max_steps=200000  #
export eval_every=100  #val every k steps
export max_seq=30

export threshold=0.3

export random_seed=1111
